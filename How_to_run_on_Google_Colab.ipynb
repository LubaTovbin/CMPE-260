{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How_to_run_on_Google_Colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf7Q_v87QNK_",
        "outputId": "6f4ef0d9-b87c-4b46-b4be-699d4ded4bff"
      },
      "source": [
        "# mount drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiOI_IaWLTVk",
        "outputId": "c1d046e4-dc8c-4862-f756-3aadc07488c9"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF9A8bIMLadA",
        "outputId": "9dd02ec1-556b-459b-9917-916550f47878"
      },
      "source": [
        "%cd drive/My\\ Drive/\n",
        "!mkdir cmpe260_proj\n",
        "%cd cmpe260_proj"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n",
            "mkdir: cannot create directory ‘cmpe260_proj’: File exists\n",
            "/content/drive/My Drive/cmpe260_proj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuUyTA7lQ-fo",
        "outputId": "e3a74923-5887-4141-948e-7bfabf29fa2a"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/cmpe260_proj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5GSZcd7Ldw2"
      },
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/drive/My\\ Drive/cmpe260_proj'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWf44GaaLqPj",
        "outputId": "8a7fc9fa-6325-499d-a558-8562e05bd4f0"
      },
      "source": [
        "!git clone https://github.com/LubaTovbin/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020'...\n",
            "remote: Enumerating objects: 749, done.\u001b[K\n",
            "remote: Total 749 (delta 0), reused 0 (delta 0), pack-reused 749\u001b[K\n",
            "Receiving objects: 100% (749/749), 37.52 MiB | 14.62 MiB/s, done.\n",
            "Resolving deltas: 100% (301/301), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QawK98-FLxkU",
        "outputId": "e1c8e3ea-7313-44ef-9e16-278bb392a79d"
      },
      "source": [
        "%cd Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/cmpe260_proj/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020\n",
            "Requirement already satisfied: numpy==1.16.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.16.4)\n",
            "Requirement already satisfied: pandas==1.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.0.3)\n",
            "Requirement already satisfied: stockstats in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.3.2)\n",
            "Requirement already satisfied: scikit-learn==0.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.21.0)\n",
            "Requirement already satisfied: gym==0.15.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (0.15.3)\n",
            "Requirement already satisfied: stable-baselines[mpi] in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (2.10.1)\n",
            "Requirement already satisfied: tensorflow==1.15.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (1.15.4)\n",
            "Requirement already satisfied: joblib==0.15.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (0.15.1)\n",
            "Requirement already satisfied: matplotlib==3.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (3.2.1)\n",
            "Requirement already satisfied: pytest<6.0.0,>=5.3.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 16)) (5.4.3)\n",
            "Requirement already satisfied: setuptools<42.0.0,>=41.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 19)) (41.6.0)\n",
            "Requirement already satisfied: wheel<0.34.0,>=0.33.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 20)) (0.33.6)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.3->-r requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.3->-r requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: int-date>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from stockstats->-r requirements.txt (line 4)) (0.1.8)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.0->-r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.15.3->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.15.3->-r requirements.txt (line 6)) (1.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.15.3->-r requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]->-r requirements.txt (line 7)) (4.1.2.30)\n",
            "Requirement already satisfied: mpi4py; extra == \"mpi\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]->-r requirements.txt (line 7)) (3.0.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 8)) (3.3.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 8)) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 8)) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 8)) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 8)) (3.12.4)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 8)) (1.12.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 8)) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 8)) (1.15.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 8)) (0.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 8)) (1.33.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.1->-r requirements.txt (line 13)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.1->-r requirements.txt (line 13)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.1->-r requirements.txt (line 13)) (0.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest<6.0.0,>=5.3.2->-r requirements.txt (line 16)) (0.2.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest<6.0.0,>=5.3.2->-r requirements.txt (line 16)) (2.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest<6.0.0,>=5.3.2->-r requirements.txt (line 16)) (20.3.0)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest<6.0.0,>=5.3.2->-r requirements.txt (line 16)) (0.13.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest<6.0.0,>=5.3.2->-r requirements.txt (line 16)) (8.6.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest<6.0.0,>=5.3.2->-r requirements.txt (line 16)) (1.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest<6.0.0,>=5.3.2->-r requirements.txt (line 16)) (20.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.3->-r requirements.txt (line 6)) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.4->-r requirements.txt (line 8)) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->-r requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->-r requirements.txt (line 8)) (3.3.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest<6.0.0,>=5.3.2->-r requirements.txt (line 16)) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG4rd6mF0Z2F",
        "outputId": "9c053dcd-516f-449c-bede-4e7499fe0f5e"
      },
      "source": [
        "!python3 run_DRL.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "   datadate  ... turbulence\n",
            "0  20090102  ...        0.0\n",
            "1  20090102  ...        0.0\n",
            "2  20090102  ...        0.0\n",
            "3  20090102  ...        0.0\n",
            "4  20090102  ...        0.0\n",
            "\n",
            "[5 rows x 12 columns]\n",
            "1053360\n",
            "[20151002 20151005 20151006 ... 20200702 20200706 20200707]\n",
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  171.0940715631016\n",
            "======Model training from:  20090000 to  20151002\n",
            "======A2C Training========\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "Training time (A2C):  1.422123674551646  minutes\n",
            "======A2C Validation from:  20151002 to  20160104\n",
            "A2C Sharpe Ratio:  0.047279655822328306\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.579593205451966  minutes\n",
            "======PPO Validation from:  20151002 to  20160104\n",
            "PPO Sharpe Ratio:  0.09953279613454485\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.4819418748219808  minutes\n",
            "======DDPG Validation from:  20151002 to  20160104\n",
            "======Trading from:  20160104 to  20160405\n",
            "previous_total_asset:1000000\n",
            "end_total_asset:1084610.6354702583\n",
            "total_reward:84610.6354702583\n",
            "total_cost:  3629.6801976062707\n",
            "total trades:  1196\n",
            "Sharpe:  0.29946278063325216\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358223\n",
            "======Model training from:  20090000 to  20160104\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.4295514146486918  minutes\n",
            "======A2C Validation from:  20160104 to  20160405\n",
            "A2C Sharpe Ratio:  0.14427806886577804\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.642440028985342  minutes\n",
            "======PPO Validation from:  20160104 to  20160405\n",
            "PPO Sharpe Ratio:  0.11693337882351139\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.4794636090596517  minutes\n",
            "======DDPG Validation from:  20160104 to  20160405\n",
            "======Trading from:  20160405 to  20160705\n",
            "previous_total_asset:1084610.6354702583\n",
            "end_total_asset:1131163.0450064929\n",
            "total_reward:46552.40953623457\n",
            "total_cost:  6382.8098781894105\n",
            "total trades:  1585\n",
            "Sharpe:  0.19400416236218074\n",
            "============================================\n",
            "turbulence_threshold:  171.0940715631016\n",
            "======Model training from:  20090000 to  20160405\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.436534857749939  minutes\n",
            "======A2C Validation from:  20160405 to  20160705\n",
            "A2C Sharpe Ratio:  0.005611250116938427\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.558828282356262  minutes\n",
            "======PPO Validation from:  20160405 to  20160705\n",
            "PPO Sharpe Ratio:  -0.11374835517238771\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.4852495511372884  minutes\n",
            "======DDPG Validation from:  20160405 to  20160705\n",
            "======Trading from:  20160705 to  20161003\n",
            "previous_total_asset:1131163.0450064929\n",
            "end_total_asset:1124372.4737545804\n",
            "total_reward:-6790.571251912508\n",
            "total_cost:  8562.783277796641\n",
            "total trades:  1734\n",
            "Sharpe:  -0.028092526119536936\n",
            "============================================\n",
            "turbulence_threshold:  171.0940715631016\n",
            "======Model training from:  20090000 to  20160705\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.4228521744410196  minutes\n",
            "======A2C Validation from:  20160705 to  20161003\n",
            "A2C Sharpe Ratio:  -0.021643538501556536\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.621805922190348  minutes\n",
            "======PPO Validation from:  20160705 to  20161003\n",
            "PPO Sharpe Ratio:  0.03527936874960779\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.4820707718531291  minutes\n",
            "======DDPG Validation from:  20160705 to  20161003\n",
            "======Trading from:  20161003 to  20170103\n",
            "previous_total_asset:1124372.4737545804\n",
            "end_total_asset:1201018.0776760795\n",
            "total_reward:76645.6039214991\n",
            "total_cost:  7974.662034626049\n",
            "total trades:  1698\n",
            "Sharpe:  0.35908186790523605\n",
            "============================================\n",
            "turbulence_threshold:  171.0940715631016\n",
            "======Model training from:  20090000 to  20161003\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.4178347826004027  minutes\n",
            "======A2C Validation from:  20161003 to  20170103\n",
            "A2C Sharpe Ratio:  0.500213471310308\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.613073535760244  minutes\n",
            "======PPO Validation from:  20161003 to  20170103\n",
            "PPO Sharpe Ratio:  0.4568423778682279\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.49261292616526287  minutes\n",
            "======DDPG Validation from:  20161003 to  20170103\n",
            "======Trading from:  20170103 to  20170404\n",
            "previous_total_asset:1201018.0776760795\n",
            "end_total_asset:1219689.0553305806\n",
            "total_reward:18670.977654501097\n",
            "total_cost:  6330.8550203579925\n",
            "total trades:  1487\n",
            "Sharpe:  0.10301076979852222\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358223\n",
            "======Model training from:  20090000 to  20170103\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.4304285724957784  minutes\n",
            "======A2C Validation from:  20170103 to  20170404\n",
            "A2C Sharpe Ratio:  0.2455256339014333\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.684579614798228  minutes\n",
            "======PPO Validation from:  20170103 to  20170404\n",
            "PPO Sharpe Ratio:  0.2670909963627783\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.4943492571512858  minutes\n",
            "======DDPG Validation from:  20170103 to  20170404\n",
            "======Trading from:  20170404 to  20170705\n",
            "previous_total_asset:1219689.0553305806\n",
            "end_total_asset:1248096.2647692144\n",
            "total_reward:28407.209438633872\n",
            "total_cost:  5773.095047431062\n",
            "total trades:  1260\n",
            "Sharpe:  0.21924581180704297\n",
            "============================================\n",
            "turbulence_threshold:  171.0940715631016\n",
            "======Model training from:  20090000 to  20170404\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.4487401207288106  minutes\n",
            "======A2C Validation from:  20170404 to  20170705\n",
            "A2C Sharpe Ratio:  0.1595451374144964\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.6652286926905315  minutes\n",
            "======PPO Validation from:  20170404 to  20170705\n",
            "PPO Sharpe Ratio:  0.36254103446890734\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.48767892519632977  minutes\n",
            "======DDPG Validation from:  20170404 to  20170705\n",
            "======Trading from:  20170705 to  20171003\n",
            "previous_total_asset:1248096.2647692144\n",
            "end_total_asset:1253100.8602816777\n",
            "total_reward:5004.5955124632455\n",
            "total_cost:  6627.483796165633\n",
            "total trades:  1367\n",
            "Sharpe:  0.035171628330074096\n",
            "============================================\n",
            "turbulence_threshold:  171.0940715631016\n",
            "======Model training from:  20090000 to  20170705\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.4350118478139242  minutes\n",
            "======A2C Validation from:  20170705 to  20171003\n",
            "A2C Sharpe Ratio:  0.08719817605926654\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.713141151269277  minutes\n",
            "======PPO Validation from:  20170705 to  20171003\n",
            "PPO Sharpe Ratio:  0.17361620006693645\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.49773286978403725  minutes\n",
            "======DDPG Validation from:  20170705 to  20171003\n",
            "======Trading from:  20171003 to  20180103\n",
            "previous_total_asset:1253100.8602816777\n",
            "end_total_asset:1368988.5948818643\n",
            "total_reward:115887.73460018658\n",
            "total_cost:  8301.00303843922\n",
            "total trades:  1628\n",
            "Sharpe:  0.6500825573233393\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358223\n",
            "======Model training from:  20090000 to  20171003\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.459873414039612  minutes\n",
            "======A2C Validation from:  20171003 to  20180103\n",
            "A2C Sharpe Ratio:  0.4828072569953544\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.763835271199544  minutes\n",
            "======PPO Validation from:  20171003 to  20180103\n",
            "PPO Sharpe Ratio:  0.39282551622802775\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.4940526326497396  minutes\n",
            "======DDPG Validation from:  20171003 to  20180103\n",
            "======Trading from:  20180103 to  20180405\n",
            "previous_total_asset:1368988.5948818643\n",
            "end_total_asset:1388285.0171002892\n",
            "total_reward:19296.422218424967\n",
            "total_cost:  2267.459548640339\n",
            "total trades:  382\n",
            "Sharpe:  0.14651124095292387\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358223\n",
            "======Model training from:  20090000 to  20180103\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.449058218797048  minutes\n",
            "======A2C Validation from:  20180103 to  20180405\n",
            "A2C Sharpe Ratio:  -0.07265373144680494\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.802549537022909  minutes\n",
            "======PPO Validation from:  20180103 to  20180405\n",
            "PPO Sharpe Ratio:  -0.05214525890968921\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.5032440821329752  minutes\n",
            "======DDPG Validation from:  20180103 to  20180405\n",
            "======Trading from:  20180405 to  20180705\n",
            "previous_total_asset:1388285.0171002892\n",
            "end_total_asset:1357508.2185820967\n",
            "total_reward:-30776.79851819249\n",
            "total_cost:  7348.337719734123\n",
            "total trades:  1166\n",
            "Sharpe:  -0.16862353564730428\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358223\n",
            "======Model training from:  20090000 to  20180405\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.458641262849172  minutes\n",
            "======A2C Validation from:  20180405 to  20180705\n",
            "A2C Sharpe Ratio:  -0.14926702127167024\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.798974080880483  minutes\n",
            "======PPO Validation from:  20180405 to  20180705\n",
            "PPO Sharpe Ratio:  -0.09617791123850558\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.50547247727712  minutes\n",
            "======DDPG Validation from:  20180405 to  20180705\n",
            "======Trading from:  20180705 to  20181003\n",
            "previous_total_asset:1357508.2185820967\n",
            "end_total_asset:1366898.5920643196\n",
            "total_reward:9390.373482222902\n",
            "total_cost:  6247.089030664606\n",
            "total trades:  977\n",
            "Sharpe:  0.0957554801674891\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358223\n",
            "======Model training from:  20090000 to  20180705\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.4916865825653076  minutes\n",
            "======A2C Validation from:  20180705 to  20181003\n",
            "A2C Sharpe Ratio:  0.03812514227334018\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.854004355271657  minutes\n",
            "======PPO Validation from:  20180705 to  20181003\n",
            "PPO Sharpe Ratio:  0.11393797394238367\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.5152772148450215  minutes\n",
            "======DDPG Validation from:  20180705 to  20181003\n",
            "======Trading from:  20181003 to  20190104\n",
            "previous_total_asset:1366898.5920643196\n",
            "end_total_asset:1373881.2732279093\n",
            "total_reward:6982.681163589703\n",
            "total_cost:  954.0182519957539\n",
            "total trades:  157\n",
            "Sharpe:  0.17698576910201008\n",
            "============================================\n",
            "turbulence_threshold:  171.0940715631016\n",
            "======Model training from:  20090000 to  20181003\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.47285662094752  minutes\n",
            "======A2C Validation from:  20181003 to  20190104\n",
            "A2C Sharpe Ratio:  -0.39319155546459816\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.883043225606283  minutes\n",
            "======PPO Validation from:  20181003 to  20190104\n",
            "PPO Sharpe Ratio:  -0.3821580300068354\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.5048640727996826  minutes\n",
            "======DDPG Validation from:  20181003 to  20190104\n",
            "======Trading from:  20190104 to  20190405\n",
            "previous_total_asset:1373881.2732279093\n",
            "end_total_asset:1486474.4957257526\n",
            "total_reward:112593.2224978432\n",
            "total_cost:  7059.811161715468\n",
            "total trades:  1469\n",
            "Sharpe:  0.3810120352404824\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358223\n",
            "======Model training from:  20090000 to  20190104\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.4601462999979655  minutes\n",
            "======A2C Validation from:  20190104 to  20190405\n",
            "A2C Sharpe Ratio:  0.12123438712528997\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.854511404037476  minutes\n",
            "======PPO Validation from:  20190104 to  20190405\n",
            "PPO Sharpe Ratio:  -0.10293036170403508\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.5049782117207845  minutes\n",
            "======DDPG Validation from:  20190104 to  20190405\n",
            "======Trading from:  20190405 to  20190708\n",
            "previous_total_asset:1486474.4957257526\n",
            "end_total_asset:1491593.7194042332\n",
            "total_reward:5119.223678480601\n",
            "total_cost:  961.9822088478891\n",
            "total trades:  142\n",
            "Sharpe:  0.3846304946122144\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358223\n",
            "======Model training from:  20090000 to  20190405\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.5114364186922709  minutes\n",
            "======A2C Validation from:  20190405 to  20190708\n",
            "A2C Sharpe Ratio:  0.1394636529082751\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.879080673058827  minutes\n",
            "======PPO Validation from:  20190405 to  20190708\n",
            "PPO Sharpe Ratio:  -0.055194543465668884\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.5130475958188375  minutes\n",
            "======DDPG Validation from:  20190405 to  20190708\n",
            "======Trading from:  20190708 to  20191004\n",
            "previous_total_asset:1491593.7194042332\n",
            "end_total_asset:1496258.5540786767\n",
            "total_reward:4664.834674443584\n",
            "total_cost:  1715.2361615687296\n",
            "total trades:  292\n",
            "Sharpe:  0.08529465610551408\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358223\n",
            "======Model training from:  20090000 to  20190708\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.5142151316006978  minutes\n",
            "======A2C Validation from:  20190708 to  20191004\n",
            "A2C Sharpe Ratio:  0.05163342321772363\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.871187643210093  minutes\n",
            "======PPO Validation from:  20190708 to  20191004\n",
            "PPO Sharpe Ratio:  -0.1412463246293476\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.5007375240325928  minutes\n",
            "======DDPG Validation from:  20190708 to  20191004\n",
            "======Trading from:  20191004 to  20200106\n",
            "previous_total_asset:1496258.5540786767\n",
            "end_total_asset:1495764.0992147967\n",
            "total_reward:-494.4548638800625\n",
            "total_cost:  263.92670311416623\n",
            "total trades:  62\n",
            "Sharpe:  -0.2847598684319717\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358223\n",
            "======Model training from:  20090000 to  20191004\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.4769219239552815  minutes\n",
            "======A2C Validation from:  20191004 to  20200106\n",
            "A2C Sharpe Ratio:  0.07796771122924799\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.869408826033275  minutes\n",
            "======PPO Validation from:  20191004 to  20200106\n",
            "PPO Sharpe Ratio:  -0.4459853108096924\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.5069283684094746  minutes\n",
            "======DDPG Validation from:  20191004 to  20200106\n",
            "======Trading from:  20200106 to  20200406\n",
            "previous_total_asset:1495764.0992147967\n",
            "end_total_asset:1486060.6879180132\n",
            "total_reward:-9703.41129678348\n",
            "total_cost:  699.9587185973381\n",
            "total trades:  159\n",
            "Sharpe:  -0.3755092946227286\n",
            "============================================\n",
            "turbulence_threshold:  96.08032158358223\n",
            "======Model training from:  20090000 to  20200106\n",
            "======A2C Training========\n",
            "Training time (A2C):  1.4719613393147786  minutes\n",
            "======A2C Validation from:  20200106 to  20200406\n",
            "A2C Sharpe Ratio:  -0.4284119741803476\n",
            "======PPO Training========\n",
            "Training time (PPO):  4.8189702987670895  minutes\n",
            "======PPO Validation from:  20200106 to  20200406\n",
            "PPO Sharpe Ratio:  -0.44622434838561476\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.512337080637614  minutes\n",
            "======DDPG Validation from:  20200106 to  20200406\n",
            "======Trading from:  20200406 to  20200707\n",
            "previous_total_asset:1486060.6879180132\n",
            "end_total_asset:1488266.5250449195\n",
            "total_reward:2205.8371269062627\n",
            "total_cost:  558.3153301572896\n",
            "total trades:  142\n",
            "Sharpe:  0.1824768370212287\n",
            "Ensemble Strategy took:  120.98450158437093  minutes\n",
            "[a8950dbd9e55:02530] *** Process received signal ***\n",
            "[a8950dbd9e55:02530] Signal: Segmentation fault (11)\n",
            "[a8950dbd9e55:02530] Signal code: Address not mapped (1)\n",
            "[a8950dbd9e55:02530] Failing at address: 0x7fb594a3f20d\n",
            "[a8950dbd9e55:02530] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fb5976e8980]\n",
            "[a8950dbd9e55:02530] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7fb5973278a5]\n",
            "[a8950dbd9e55:02530] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7fb597b92e44]\n",
            "[a8950dbd9e55:02530] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7fb597328735]\n",
            "[a8950dbd9e55:02530] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7fb597b90cb3]\n",
            "[a8950dbd9e55:02530] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpc_3Kpvbj--"
      },
      "source": [
        " 1. To make a Short State Vector. remove technical indicators in\r\n",
        "     preprocessing/ preprocessors.py\r\n",
        "     line 102 write \r\n",
        "     df_final=df_preprocess\r\n",
        "\r\n",
        " 2. To change MlpPolicy to MlpLstmPolicy for A2C, in model/models.py,\r\n",
        "    line 32:\r\n",
        "    replace model = A2C('MlpPolicy', env_train, verbose=0)\r\n",
        "    with model = A2C('MlpLstmPolicy', env_train, verbose=0)\r\n",
        "\r\n",
        " 3. To change strategy during turbulence, modify\r\n",
        "    EnvMultipleStock_trade.py \r\n",
        "    EnvMultipleStock_validation.py\r\n",
        "\r\n",
        " 4. To evaluate with Sortino ratio:\r\n",
        "    in models.py in get_validation_sharpe\r\n",
        "    replace lines 147, 148 with the following:\r\n",
        "    luba_df = df_total_value[df_total_value['daily_return']<0]\r\n",
        "    sharpe = (4 ** 0.5) * df_total_value['daily_return'].mean() / \\\r\n",
        "    luba_df['daily_return'].std()\r\n",
        "\r\n",
        " 5. To run Multi-PPO, in model/models.py,\r\n",
        "    line 32: \r\n",
        "    replace model = A2C('MlpPolicy', env_train, verbose=0) with \r\n",
        "    model = PPO2('MlpPolicy', env_train, ent_coef = 0.005, nminibatches = 8)\r\n",
        "    line 60: \r\n",
        "    replace model = DDPG('MlpPolicy', env_train, param_noise=param_noise, action_noise=action_noise) \r\n",
        "    with model = PPO2('MlpPolicy', env_train, ent_coef = 0.005, nminibatches = 8)\r\n"
      ]
    }
  ]
}